{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE576_data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngdodd/transformers/blob/master/CSE576_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq2GHCExDrpH"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqHTuH3mDlNk",
        "outputId": "7cee6369-d8ee-43e0-fa64-1d26b75ddc81"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Sm3plSDvmv"
      },
      "source": [
        "Python script for preprocessing the original swag + cosmos_qa datasets. Run this to make the functions available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XphIgdoTDxqK"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 23 19:18:39 2020\n",
        "\n",
        "@author: nickg\n",
        "\"\"\"\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "    \n",
        "# Write a single json'd data entry to file\n",
        "def write_jsonl_entry(entry, jsonl_file):\n",
        "    json.dump(entry, jsonl_file)\n",
        "    jsonl_file.write('\\n')\n",
        "    \n",
        "# A two-for-one data formatter for both swag and hellaswag datasets.\n",
        "# Splits for swag: train, val\n",
        "# Splits for hellaswag: train, validation\n",
        "def swag2quail(split, prefix=\"\"):\n",
        "    is_hella = prefix==\"hella\"\n",
        "    itr_container = pd.read_csv(\"swag/{}.csv\".format(split)).sample(frac=1).reset_index(drop=True).iterrows() \\\n",
        "                                              if not is_hella else enumerate(load_dataset('hellaswag')[split].shuffle()) \n",
        "    elem_indices = ['source_id', 'ctx_a', 'ctx_b', 'label'] if is_hella else ['fold-ind', 'sent1', 'sent2', 'label']\n",
        "    ending_funct = lambda e, k : e['endings'][k] if is_hella else e['ending{}'.format(k)]\n",
        "    \n",
        "    with open(\"swag/{}swag_{}.jsonl\".format(prefix, split), mode='w', encoding='utf-8') as f:\n",
        "        for id_, swag_entry in tqdm(itr_container):\n",
        "            quail_entry = {\"id\": str(swag_entry[elem_indices[0]]),\n",
        "                           \"context\": swag_entry[elem_indices[1]],\n",
        "                           \"question\": swag_entry[elem_indices[2]],\n",
        "                           \"question_type\": 'Subsequent_state',\n",
        "                           \"answers\": [ending_funct(swag_entry,k) for k in range(4)],\n",
        "                           \"correct_answer_id\": str(swag_entry[elem_indices[-1]]) }\n",
        "            write_jsonl_entry(quail_entry, f)\n",
        "         \n",
        "# Convert cosmos_qa to quail format. Questions for which the correct answer contains\n",
        "# \"None of the above\" are unanswerable questions in this dataset.\n",
        "# Splits: train, validation\n",
        "def cosmos2quail(split):\n",
        "    is_unanswerable = lambda e : \"None of the above\" in e[\"answer{}\".format(e['label'])]\n",
        "    cosmos = load_dataset('cosmos_qa')[split].shuffle()\n",
        "\n",
        "    with open(\"cosmos_qa/cosmos_qa_{}.jsonl\".format(split), mode='w', encoding='utf-8') as f:\n",
        "        for cosmos_entry in tqdm(cosmos):\n",
        "            quail_entry = {\"id\": str(cosmos_entry['id']),\n",
        "                           \"context\": cosmos_entry['context'],\n",
        "                           \"question\": cosmos_entry['question'],\n",
        "                           \"question_type\": 'Unanswerable' if is_unanswerable(cosmos_entry) else 'Causality',\n",
        "                           \"answers\": [cosmos_entry['answer{}'.format(k)] for k in range(4)],\n",
        "                           \"correct_answer_id\": cosmos_entry['label'] }\n",
        "            write_jsonl_entry(quail_entry, f)\n",
        "    \n",
        "def process_dataset(dataset, split):\n",
        "    print(\"\\nProcessing {}[{}]...\".format(dataset, split))\n",
        "    if 'swag' in dataset:\n",
        "        swag2quail(split, dataset.split('swag')[0])\n",
        "    elif 'cosmos_qa' in dataset:\n",
        "        cosmos2quail(split)\n",
        "    else:\n",
        "        print(\"Unknown dataset: {}\".format(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01cDjh-NEDBY"
      },
      "source": [
        "Get quail formatted cosmos_qa datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIeLK7VpEDtb",
        "outputId": "6e5bae52-1e56-43a4-fdc3-52b262981493"
      },
      "source": [
        "!mkdir cosmos_qa\n",
        "process_dataset(dataset='cosmos_qa', split='train')\n",
        "process_dataset(dataset='cosmos_qa', split='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘cosmos_qa’: File exists\n",
            "\n",
            "Processing cosmos_qa[train]...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset cosmos_qa (/root/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/e539f7f30a86d4fa42c3faf36515b9662ee56c3b62f2c14d81c8f4e8e3a64b5f)\n",
            "100%|██████████| 25262/25262 [00:02<00:00, 9314.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing cosmos_qa[validation]...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset cosmos_qa (/root/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/e539f7f30a86d4fa42c3faf36515b9662ee56c3b62f2c14d81c8f4e8e3a64b5f)\n",
            "100%|██████████| 2985/2985 [00:00<00:00, 9088.55it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Iup_lEEXTt"
      },
      "source": [
        "Download SWAG dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYl0oHmHEZ7D",
        "outputId": "2244cd42-3c66-4839-9624-dd3843ccb89b"
      },
      "source": [
        "!mkdir swag\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/test.csv -O swag/test.csv\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv -O swag/train.csv\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/val.csv -O swag/val.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘swag’: File exists\n",
            "--2020-11-24 06:43:38--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7817885 (7.5M) [text/plain]\n",
            "Saving to: ‘swag/test.csv’\n",
            "\n",
            "swag/test.csv       100%[===================>]   7.46M  28.8MB/s    in 0.3s    \n",
            "\n",
            "2020-11-24 06:43:39 (28.8 MB/s) - ‘swag/test.csv’ saved [7817885/7817885]\n",
            "\n",
            "--2020-11-24 06:43:39--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28243333 (27M) [text/plain]\n",
            "Saving to: ‘swag/train.csv’\n",
            "\n",
            "swag/train.csv      100%[===================>]  26.93M  50.7MB/s    in 0.5s    \n",
            "\n",
            "2020-11-24 06:43:40 (50.7 MB/s) - ‘swag/train.csv’ saved [28243333/28243333]\n",
            "\n",
            "--2020-11-24 06:43:40--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7893588 (7.5M) [text/plain]\n",
            "Saving to: ‘swag/val.csv’\n",
            "\n",
            "swag/val.csv        100%[===================>]   7.53M  26.1MB/s    in 0.3s    \n",
            "\n",
            "2020-11-24 06:43:41 (26.1 MB/s) - ‘swag/val.csv’ saved [7893588/7893588]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxFTLhNwEa5g"
      },
      "source": [
        "Get quail formatted swag and hellaswag datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pllAWfIEjun",
        "outputId": "8c874a45-1380-4fe3-e45b-210df01c3739"
      },
      "source": [
        "# Swag - not available in huggingface/datasets. CSV files downloaded from github into ./swag\n",
        "data = process_dataset(dataset='swag', split='train')\n",
        "process_dataset(dataset='swag', split='val')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing swag[train]...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "73546it [00:12, 5857.67it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing swag[val]...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "20006it [00:03, 5754.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G--UmDFwVGhJ",
        "outputId": "12e9536b-e292-49f5-9dac-f1cb9f8ad44e"
      },
      "source": [
        "# Hellaswag is available directly from huggingface/datasets. Ref: https://rowanzellers.com/hellaswag/\n",
        "process_dataset(dataset='hellaswag', split='train')\n",
        "process_dataset(dataset='hellaswag', split='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing hellaswag[train]...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset hellaswag (/root/.cache/huggingface/datasets/hellaswag/default/0.1.0/7fc3b0cd8d8ca874131456256c38a34e5d50a9416e63233aaea8af9636a44212)\n",
            "39905it [00:05, 7580.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing hellaswag[validation]...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset hellaswag (/root/.cache/huggingface/datasets/hellaswag/default/0.1.0/7fc3b0cd8d8ca874131456256c38a34e5d50a9416e63233aaea8af9636a44212)\n",
            "10042it [00:01, 7561.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAceRTYeVzwi"
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import random\n",
        "\n",
        "train = []\n",
        "n_final_train_samples = 20000\n",
        "\n",
        "with open('cosmos_qa/cosmos_qa_train.jsonl','r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip('\\n')\n",
        "      train.append(line)\n",
        "\n",
        "with open('swag/hellaswag_train.jsonl','r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip('\\n')\n",
        "      train.append(line)\n",
        "\n",
        "with open('swag/swag_train.jsonl','r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip('\\n')\n",
        "      train.append(line)\n",
        "\n",
        "random.shuffle(train)\n",
        "with open('train.jsonl','w') as w:\n",
        "  for itr, entry in enumerate(train):\n",
        "    if itr >= n_final_train_samples:\n",
        "      break\n",
        "    entry = json.loads(entry)\n",
        "    entry['correct_answer_id'] = str(entry['correct_answer_id'])\n",
        "    w.write(json.dumps(entry))\n",
        "    w.write('\\n')\n",
        "\n",
        "val = []\n",
        "n_final_val_samples = 10000\n",
        "\n",
        "with open('cosmos_qa/cosmos_qa_validation.jsonl','r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip('\\n')\n",
        "      val.append(line)\n",
        "\n",
        "with open('swag/hellaswag_validation.jsonl','r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip('\\n')\n",
        "      val.append(line)\n",
        "\n",
        "with open('swag/swag_val.jsonl','r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip('\\n')\n",
        "      val.append(line)\n",
        "\n",
        "random.shuffle(val)\n",
        "with open('val.jsonl','w') as w:\n",
        "  for itr, entry in enumerate(val):\n",
        "    if itr >= n_final_val_samples:\n",
        "      break\n",
        "    entry = json.loads(entry)\n",
        "    entry['correct_answer_id'] = str(entry['correct_answer_id'])\n",
        "    w.write(json.dumps(entry))\n",
        "    w.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}